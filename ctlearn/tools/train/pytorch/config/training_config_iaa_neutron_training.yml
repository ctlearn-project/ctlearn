data: 

  train_gamma_proton: ./data/gamma_proton_train_remix.dl1.pickle #gamma_proton_reduced_train.pickle #./data/gamma_proton_1910000_train.pickle 
  validation_gamma_proton: ./data/gamma_proton_212282_validation.pickle 
  # validation_gamma_proton: ./data/gamma_proton_1910000_train.pickle 

  train_gamma: ./data/gamma_955000_train.pickle 
  validation_gamma: ./data/gamma_106141_validation.pickle 

  test_gamma: ./data/gamma_1805522_test_gamma.pickle 
  test_proton: ./data/proton_130811_test_proton.pickle 
  test_electron: None
  test_validation_gamma: ./data/gamma_180552_test_val_gamma.pickle 

  test_validation_gamma_proton: ./data/gamma_proton_212282_validation.pickle 
  # test_validation_gamma_proton: ./data/gamma_proton_1910000_train.pickle 

  observation: ./run_2931.dl1.pickle 
  # Important: This is only for testing purpose. Set always to 0
  # when you are training, validating or estimating the dl2 files
  training_reduce_factor: 0 #64 #4
  validation_reduce_factor: 0 #16 #8
  validation_test_reduce_factor: 0 #16 #8

  # Check points
  type_checkpoint: /storage/ctlearn_data/check_points/v_0_3/type_train_acc_81.1937808990478516.pth # ./run/run_type_train_14/exp_14_type_train/version_0/Epoch_27_type_train_acc_81.0595095157623291.pth #./run/run_type_train_2/exp_2_type_train/version_4/type_train_acc_81.1937808990478516.pth #./run/run_type_train_1/type_validation_accuracy_81.6743765368707670.pth #./run/run_type_train_2/exp_2_type_train/version_4/type_train_acc_81.1937808990478516.pth
  energy_checkpoint: /storage/ctlearn_data/check_points/v_0_3/energy_train_loss_9.7920012821687248.pth
  direction_checkpoint: /storage/ctlearn_data/check_points/v_0_3/Epoch_42_direction_train_loss_-2739.9621516285983489.pth #./run/run_direction_train_2/exp_2_direction_train/version_1/direction_train_loss_1.0721765518581907.pth 
  
run_details:

    mode: "train" # The option are: "train", "results", "observation" and "validate"
    task: "type" # The option are: "all", "energy" "type" and "direction"
    test_type: "gamma" # The option are: "gamma" "proton" or "electron" 
    experiment_number: 14 # The experiment number. The experiment folder is saved into the "run" folder.


cut-off:

  leakage_intensity: 0.2 # bigger to this value, the event is removed 
  intensity: 50 # below to this value, the event is removed 

model:  

  model_type: 
    model_name: "DoubleBBEfficientNet"
    parameters:
      model_variant: "efficientnet-b3"
      task: 'type'
      num_outputs: 2
      device_str: "cuda"
      energy_bins: None

  # model_type:
  #   model_name: "ThinResNet_DBB"
  #   parameters:
  #     task: 'type'
  #     num_inputs: 1
  #     num_outputs: 2
  #     num_blocks: [2, 3, 3, 3]
  #     dropout: 0.1 
  #     use_bn: False  

  model_energy:
    model_name: "ThinResNet_DBB"
    parameters:
      task: 'energy'
      num_inputs: 1
      num_outputs: 1
      num_blocks: [3, 4, 6, 3] #[2, 3, 3, 3]
      dropout: 0.1 
      use_bn: False  
    

  model_direction: 
    model_name: "ThinResNet_DBB"
    parameters:
      task: 'direction'
      num_inputs: 1
      num_outputs: 3
      num_blocks: [3, 4, 6, 3]
      dropout: 0.1 
      use_bn: False  
      

# Hyper-parameters
hyp: 

  epochs: 12
  batches: 128 #64
  dynamic_batches: True
  optimizer: Adamw
  momentum: 0.957 #Yolo 0.937 # Efficient-b3 0.757
  weight_decay: 0.0005 #0.004676 #0.0001 #0.00002 Efficient-b3 0.0005
  learning_rate: 1e-6 #1e-5 #Efficient-b3 1e-5
  lrf: 0.1
  start_epoch: 0
  steps_epoch: 100 # Computed online. Must be removed
  l2_lambda: 1e-5 #1e-5 #1e-5 # L2 regularization (Set to 0.0 to skip the L2 Regularization)
  adam_epsilon: 1.0e-08 #7.511309034256153e-05 #1.0e-08
  gradient_clip_val: 2.0 # Avoid gradient explosion

  save_k: 200 # Save as maximum k checkpoints.

augmentation:
  # probabilities for augmentation range = [0, 1.0] 
  # prob = 0.0 -> Always apply the augmentation
  # prob >= 1.0 -> Never apply the augmentation, i.e., Set bigger than 1.0 ( ex: 2.0) if you want disable it.
  # Note: mask augmentation is always on even with flag use_augmentation = True
  # To disable it, just set to 2.5 for example.

  use_augmentation: True  # This apply only on training mode.
  aug_prob: 0.5 # Probability of use Augmentation
  rot_prob: 0.5 # Rotation probability
  trans_prob: 0.5 # Translation probability
  flip_hor_prob: 0.5 # Horizontal Flip probability
  flip_ver_prob: 0.5 # Vertical Flip probability
  mask_prob: 0.5 # Apply mask probability
  mask_dvr_prob: 0.5 # Apply dvr mask probability
  noise_prob: 0.5 # No implemented yet.
  max_rot: 5 # Maximum rotation in augmentation
  max_trans: 10 # Maximum translation in augmentation

normalization: 

  # Normalization: Im' = (Im-mu)/sigma
  use_clean: True # Use the image with the applied mask (True), IOC the mask is not applied (False)
  use_clean_dvr: False
  type_mu: 0.0
  type_sigma: 1000.0

  dir_mu: 0.0
  dir_sigma: 1000.0

  energy_mu: 0.0
  energy_sigma: 1000.0
  
dataset:
  num_workers: 1 # 
  pin_memory: True 
  persistent_workers: True # 

# Hardware Architecture and precision
arch: 
  # device: 'mps' # Apple Mx
  device: 'cuda'
  precision_type: "32-true" # Options: "64-true" "32-true" "16-true" "16-mixed" "bf16-mixed" "bf16-true" 
  precision_energy: "32-true" # Options:  "64-true" "32-true" "16-true" "16-mixed" "bf16-mixed" "bf16-true" 
  precision_direction: "32-true" # "bf16-mixed" # Options:  "64-true" "32-true" "16-true" "16-mixed" "bf16-mixed" "bf16-true" 
                                    # (bf16 for GPU with Ampere or higher, it is better that 16 because is numerical more stability)
  # devices: [0,1] # [0,1] For multiple GPUs
  devices: [0,1]
  # Note: Check the documentation for more information.
  strategy: 'deepspeed_stage_2' # Options: auto, dpp, dpp_swap, fsdp, deepspeed, horovod, bagua, deepspeed_stage_2, deepspeed_stage_3, colossalai, hivemind, etc...

Notes:
  Note_1: Training with augmentation dvr using 1-3 dilatations
  Note_2: Trainining b3 applying always the mask