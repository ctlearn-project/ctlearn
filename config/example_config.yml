Logging:
    # Required string.
    # Path to directory to store TensorFlow model checkpoints and summaries.
    # A timestamped copy of the configuration file will also be put here.
    model_directory: '/tmp/logs/example/'

Data:
    # Required string. 
    # Valid options:
    #   - 'HDF5': ImageExtractor v0.5.1 HDF5 format
    # Data file format. Used to choose the right DataLoader subclass.
    format: 'HDF5'

    # Required string.
    # Path to a file listing data file paths, one per line.
    file_list: '/tmp/dataset/file_list.txt'

    # Optional Boolean. Default: true
    # Whether to apply processing to loaded data. If False, no DataProcessor
    # is instantiated and Data:Processing settings are not used.
    # Useful for quickly turning off all data processing, e.g., when debugging.
    apply_processing: true
    
    # Settings for instantiating the DataLoader.
    Loading:
        # Optional string. Default: 'array'
        # Valid options:
        #   - 'array': Examples are all images of a telescope type per event
        #   - 'single_tel': Examples are single telescope images
        # How to construct examples from the data.
        example_type: 'array'

		# Optional Boolean. Default: False
		# If False, only a channel of integrated energy pulse per pixel is loaded 
		# If True, an additional channel is loaded indicated peak pulse arrival time per pixel
		use_peak_times: False

        # Optional string. Default: 'LST'
        # Valid options:
        #   - 'LST': LST Telescope and Camera
        #   - 'MSTF': MST Telescope with FlashCAM Camera
        #   - 'MSTN': MST Telescope with NectarCAM Camera
        #   - 'MSTS': SCT Telescope and Camera
        #   - 'SST1': SST-1M Telescope with DigiCam Camera
        #   - 'SSTA': SST-2M ASTRI Telescope and Camera
        #   - 'SSTC': SST-2M GCT Telescope with CHEC Camera
        #   - 'VTS': VERITAS Telescope and Camera
        # Telescope type to load.
        selected_tel_type: 'LST'
        
        # Optional list of integers. Default: load all telescopes of the
        # selected tel type.
        # ID numbers of telescopes to load of the selected tel type.
        selected_tel_ids: [1, 2, 3, 4]

        # Optional integer. Default: 1
        # Minimum number of triggered telescopes of the selected type which
        # must be present for an event to be loaded.
        min_num_tels: 1

        # Optional string. Default: don't apply any cuts.
        # PyTables cut condition to apply selection cuts to events.
        # See https://www.pytables.org/usersguide/condition_syntax.html
        # for explanation of syntax. Use ViTables http://vitables.org/ to
        # examine HDF5 files to see valid attribute names.
        cut_condition: '(mc_energy > 1.0) & (h_first_int < 20000)'

        # Optional float. Default: 0.1
        # Randomly chosen fraction of data to set aside for validation. 
        validation_split: 0.1

        # Optional integer. Default: use default random initialization.
        # Seed for randomly splitting data into training and validation sets.
        seed: 1234
    
    # Settings for instantiating the DataProcessor.
    Processing:
        # Optional Boolean. Default: false
        # Whether to crop each telescope image by cleaning the image,
        # calculating a bounding box about the cleaned image centroid, and
        # returning the original or cleaned image cropped about the box.
        crop: false

        # Optional string or null. Default: 'twolevel'
        # Valid options:
        #   - 'twolevel': Use absolute two-level cleaning algorithm
        #   - null: Don't apply any cleaning
        # When cropping, type of image cleaning to apply when calculating
        # the weighted centroid position of each telescope image.
        image_cleaning: 'twolevel'

        # Optional Boolean. Default: false
        # When cropping, whether to return the cleaned instead of original
        # cropped images.
        return_cleaned_images: false
       
        # Optional dictionary with string keys and integer values.
        # See Data:Loading:selected_tel_type for valid keys.
        # Default:
        #   'MSTS': 48
        # When cropping, side length of the square cropped image.
        bounding_box_sizes:
            'MSTS': 48
             
        # Optional dictionary with string keys and list of two integers values.
        # See Data:Loading:selected_tel_type for valid keys.
        # Default:
        #   MSTS: [5.5, 1.0]
        # When cropping and using two level cleaning method, absolute cleaning
        # thresholds to use. The first value is the picture threshold,
        # which passes all pixels in the image greater than it, and the
        # second is the boundary threshold, which adds all pixels greater
        # than it and adjacent to previously selected pixels.
        thresholds:
            'MSTS': 
                - 5.5
                - 1.0

        # Optional string or null. Default: null
        # Valid options:
        #   - 'log': Log-normalize by subtracting from each pixel the minimum
        #       value from all selected images, then taking the natural log
        #   - null: Don't apply any normalization
        # Normalization method to be applied pixel-wise to each image.
        normalization: null

        # Optional string or null. Default: null
        # Valid options:
        #   - 'trigger': Order by all triggered followed by all blank images
        #   - 'size': Order by largest to smallest total sum of pixel values
        #   - null: Keep DataLoader ordering (by telescope ID)
        # How to sort telescope images when loading data in array mode.
        sort_images_by: null
   
    # Settings for the TensorFlow Estimator input function.
    Input:
        # Required integer.
        # Number of examples per batch for training/prediction.
        batch_size: 64

        # Required Boolean.
        # Whether to prefetch examples in the TensorFlow Dataset.
        prefetch: true
    
        # Required integer.
        # If prefetching, maximum number of examples to prefetch and store
        # in buffer.
        prefetch_buffer_size: 10

        # Required Boolean.
        # Whether to apply Dataset.map(). Must be true for HDF5 data.
        map: true

        # Required integer.
        # Number of parallel threads to use when loading examples.
        num_parallel_calls: 1

        # Required Boolean.
        # Whether to shuffle examples when creating the TensorFlow Dataset.
        shuffle: true
        
        # Required integer.
        # Number of examples to shuffle at once. Larger buffer sizes
        # give a more uniform shuffle at the cost of a longer time to shuffle.
        shuffle_buffer_size: 10000

# Settings for mapping 1D pixel vectors to 2D images.
Image Mapping:
    # Optional string. Default: 'oversampling'
    # Valid options:
    #   - 'oversampling': Convert each each hexagonal pixel to four square
    #       pixels and reweight the charge to realign to a square grid
    # Algorithm to convert images with hexagonal pixel grids to square grids
    hex_conversion_algorithm: 'oversampling'

    # Optional dictionary with string keys and integer values.
    # See Data:Loading:selected_tel_type for valid keys.
    # Default: 0 for every valid key.
    # Number of pixels of padding to add around the edge of the mapped image.
    # A value of 1 will increase both the image height and width by 2, etc.
    padding:
        'LST': 2
        'MSTF': 1
        'MSTN': 2
        'MSTS': 4
        'SST1': 1
        'SSTA': 0
        'SSTC': 0
        'VTS': 1

# Settings for the TensorFlow model.
Model:

    # Optional string or null. Default if missing or null is to load
    # CTLearn default models directory: 'ctlearn/ctlearn/default_models/'
    # Path to directory containing model module.
    model_directory: '/my/model/path/'

    # Required list of two strings.
    # Module in model directory and function in module implementing the model.
    # Module and function pairs included with CTLearn:
    #   - {module: 'single_tel', function: 'single_tel_model'}: single
    #       tel model using a basic convolutional neural network (CNN)
    #   - {module: 'cnn_rnn', function: 'cnn_rnn_model'}: array model
    #       feeding output of a basic CNN for each  telescope into a
    #       recurrent neural network (RNN)
    #   - {module: 'variable_input_model', function: 'variable_input_model'}:
    #       array model feeding combined outputs of a CNN for each
    #       telescope into a CNN network head
    model: {module: 'cnn_rnn', function: 'cnn_rnn_model'}
    
    # The options in this section are combined with the dataset metadata
    # and passed as a dictionary called "params" to the model function.
    # The config option 'model_directory' above is included in params as well.
    # None of these options are accessed anywhere outside the model, so
    # arbitrary options may be added here. This permits custom configuration
    # of user-provided models.
    # Below are listed the configuration options for the CTLearn included 
    # models, which are only accessed if using those models.
    Model Parameters:
        single_tel:
            # Required dict with keys 'module' and 'function' & string values.
            # Module and function for image classification network.
            # Valid options:
            #   - {module: 'basic', function: 'conv_block'}
            network: {module: 'basic', function: 'conv_block'}

            # Required string or null.
            # Path to a checkpoint file or model directory from which to load
            # pretrained network weights. If null, don't load any weights.
            pretrained_weights: null

        cnn_rnn:
            # Required dict with keys 'module' and 'function' & string values.
            # Module and function for single telescope CNN.
            # Valid options:
            #   - {module: 'basic', function: 'conv_block'}
            cnn_block: {module: 'basic', function: 'conv_block'}
            
            # Required string or null.
            # Path to a checkpoint file or model directory from which to load
            # pretrained CNN block weights. If null, don't load any weights.
            pretrained_weights: null
            
            # Optional float. Default: 0.5
            # Dropout rate of dropout layers. 
            dropout_rate: 0.5

        variable_input_model:
            # Required dict with keys 'module' and 'function' & string values.
            # Module and function for single telescope CNN.
            # Valid options:
            #   - {module: 'basic', function: 'conv_block'}
            cnn_block: {module: 'basic', function: 'conv_block'}
            
            # Required string.
            # Valid options:
            #   - 'vector': Use for network heads expecting an input vector 
            #   - 'feature_maps': Use for networks heads expecting input as
            #       a stack of feature maps as used for convolutional layers
            telescope_combination: 'feature_maps'

            # Required dict with keys 'module' and 'function' & string values.
            # Module and function for network head.
            # Valid options:
            #   - {module: 'basic', function: 'fc_head'}
            #   - {module: 'basic', function: 'conv_head'}
            network_head: {module: 'basic', function: 'conv_head'}
            
            # Required string or null.
            # Path to a checkpoint file or model directory from which to load
            # pretrained CNN block weights. If null, don't load any weights.
            pretrained_weights: null

        basic:
            conv_block:
                # Required list of dicts with keys 'filters' and 'kernel_size'
                # with integer values.
                # Filter dimension and kernel size for the CNN block
                # convolutional layers.
                layers:
                    - {filters: 32, kernel_size: 3}
                    - {filters: 64, kernel_size: 3}
                    - {filters: 128, kernel_size: 3}
                
                # Required dictionary with keys 'size' and 'strides' and
                # integer values, or null.
                # Max pool size and strides. If null, don't perform any pooling.
                max_pool: {size: 2, strides: 2}
                
                # Required integer or null.
                # Number of output filters of a final 1x1 convolutional layer.
                # If null, don't include this bottleneck layer.
                bottleneck: null
                
                # Optional Boolean. Default: false
                # Whether to include a batch normalization layer after each
                # convolutional layer. Exercise caution when using with
                # array-level models.
                batchnorm: false

            fc_head:
                # Required list of integers.
                # Number of units for each fully-connected head dense layer.
                layers: [1024, 512, 256, 128, 64]
                
                # Optional Boolean. Default: false
                # Whether to include a batch normalization layer after each
                # dense layer.
                batchnorm: false

            conv_head:
                # Required list of dicts with keys 'filters' and 'kernel_size'
                # with integer values.
                # Filter dimension and kernel size for the convolutional
                # network head layers.
                layers:
                    - {filters: 64, kernel_size: 3}
                    - {filters: 128, kernel_size: 3}
                    - {filters: 256, kernel_size: 3}
                
                # Optional Boolean. Default: true
                # Whether to perform average pooling over spatial dimensions
                # of the convolutional output before calculating final output.
                final_avg_pool: true
                
                # Optional Boolean. Default: false
                # Whether to include a batch normalization layer after each
                # convolutional layer.
                batchnorm: false
            
            # Optional float. Default: 0.99
            # Decay parameter for batch normalization layers.
            batchnorm_decay: 0.99

Training:

    # Required integer.
    # Number of validations made before finishing training. If 0, run forever.
    num_validations: 0

    # Required integer.
    # Number of training steps to run before each evaluation on the validation set.
    num_training_steps_per_validation: 1000

    Hyperparameters:
        # Required string.
        # Valid options: ['Adadelta', 'Adam', 'RMSProp', 'SGD']
        # Optimizer function for training.
        optimizer: 'Adam'
        
        # Optional float. Required if optimizer is 'Adam', ignored otherwise.
        # Epsilon parameter for the Adam optimizer.
        adam_epsilon: 1.0e-8
        
        # Required integer.
        # Base learning rate before scaling or annealing.
        base_learning_rate: 0.001

        # Required Boolean.
        # Whether to scale the learning rate inversely proportional to the
        # number of triggered telescopes. Not used for single tel models.
        scale_learning_rate: false

        # Required Boolean.
        # Whether to weight the loss to rebalance unequal classes.
        apply_class_weights: false

        # Required string or null. If provided, train on only variables
        # within the scope matching this name, freezing all others. This is 
        # useful when loading pretrained weights. If null, train on all
        # trainable variables.
        variables_to_train: null

Prediction:
    # Required Boolean if running in predict mode, ignored otherwise.
    # Whether the data files contain the true classification
    # values. Generally true for simulations and false for actual data.
    true_labels_given: true

    # Optional Boolean. Default: false
    # Whether to export predictions as a CSV file.
    export_as_file: true

    # Required Boolean if running in predict mode and export_as_file is true,
    # ignored otherwise.
    # Path to file to save predictions.
    prediction_file_path: '/tmp/mypredictions.csv'

TensorFlow:
    # Optional Boolean. Default: false
    # Whether to run TensorFlow debugger.
    run_TFDBG: false

# Optional: used in run_multiple_configurations.py only
Multiple Configurations Settings:
    # Required string.
    # Path to file to save configuration combination for each run for reference.
    run_combinations_path: '/tmp/run_combinations.yml'

    # Optional integer. Default: 1
    # Number of grouped range values to generate. All range values with null
    # num_values are grouped so that random values are generated for each
    # combination of parameters, not for each parameter separately. Ignored for
    # ungrouped range values, that is, where num_values is specified.
    num_grouped_range_values: 5

# Optional: used in run_multiple_configurations.py only
# Each item in this section has the form 
# <config_nickname>:
#   config: list of the keys to access the config parameter in config file
#   value_type: one of
#       - 'list': values is a list of possible values
#       - 'range': values is a dictionary as described below
#       - 'grouped': values is a dictionary of <group_name>: value
#   values: list or dictionary as specified by the value_type
#
# The values section for a config option with value_type 'range' has the form:
# values:
#   lower_bound: Required integer. Lower bound of the range.
#   upper_bound: Required integer. Upper bound of the range.
#   spacing: Required string. Valid options:
#       - 'log': generate values uniformly using logarithmic spacing
#       - 'linear': generate values uniformly using linear spacing
#   selection: Required string. Valid options:
#       - 'random': Select values at random from a uniform distribution
#       - 'grid': Select values from an evenly-spaced grid
#   num_values: Required integer or null. Number of values to generate. All 
#       range configs with num_values null are grouped together with
#       num_grouped_range_values the total number of values generated. The
#       recommended usage in most cases is to specify num_values for grid
#       selection and set to null for random selection.
Multiple Configurations Values:
    learning_rate:
        config: ['Training', 'Hyperparameters', 'base_learning_rate']
        value_type: 'range'
        values:
            lower_bound: 1.0e-6
            upper_bound: 1.0e-3
            spacing: 'log'
            selection: 'random'
            num_values: null
    multiplicity_cut:
        config: ['Data', 'Loading', 'min_num_tels']
        value_type: 'list'
        values:
            - 1
            - 2
            - 3
            - 4
    telescope_type:
        config: ['Data', 'Loading', 'selected_tel_type']
        value_type: 'grouped'
        values:
            LST: 'LST'
            MSTN: 'MSTN'
    selected_telescopes:
        config: ['Data', 'Loading', 'selected_tel_ids']
        value_type: 'grouped'
        values:
            LST: [1, 2, 3, 4]
            MSTN: [30, 31, 32, 33]
    network_type:
        config: ['Model', 'model']
        value_type: 'grouped'
        values:
            CNN_RNN:
                module: 'cnn_rnn'
                function: 'cnn_rnn_model'
            variable_input:
                module: 'variable_input_model'
                function: 'variable_input_model'
    telescope_sorting:
        config: ['Data', 'Processing', 'sort_images_by']
        value_type: 'grouped'
        values:
            CNN_RNN: 'size'
            variable_input: null
