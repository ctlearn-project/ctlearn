# Example config file containing all possible options for CTLearn, whether
# they're required or optional, their datatypes, and default arguments.
# A config file is only required for creating new models. Existing CTLearn
# benchmark models can be constructed from the command line. Please see the
# README for instructions.
#
# For an explanation of the data format and valid column names for the
# selection_string, array_info, and event_info Data config options, see
# https://github.com/cta-observatory/dl1-data-handler/wiki/CTA-ML-Data-Format
# or use ViTables http://vitables.org/ to examine the HDF5 files directly.

# List of reconstruction tasks. For now it's recommended to you single task learning.
# Valid options:
#     - 'type'
#     - 'energy'
#     - 'direction'
# These options have to be in consonance with the 'event_info' and 
# 'transforms' in 'Data'.
# Requirements/recommendations:
#   - 'type':
#       event_info:
#           - 'true_shower_primary_id'
#       transforms:
#           -  name: 'ShowerPrimaryID'
#
#   - 'energy':
#       event_info:
#           - 'true_energy'
#       transforms:
#           -  name: 'MCEnergy'
#
#   - 'direction':
#       event_info:
#           - 'true_energy'
#           - 'true_alt'
#           - 'true_az'
#       transforms:
#           -  name: 'MCEnergy'
#           - name: 'DeltaAltAz'
#
# Required list of strings.
Reco: ['energy']

Logging:
    # Path to directory to store TensorFlow model checkpoints and summary plots.
    # A timestamped copy of the configuration file will also be put here.
    # Required string.
    model_directory: '/tmp/logs/example/'

Data:
    # Either a list of data file paths,
    # or a path to a file listing data file paths, one per line.
    # Required list or string.
    file_list: '/tmp/dataset/file_list.txt'

    # Type of examples to load.
    # Optional string. Default: 'mono'
    # Valid options:
    #   - 'mono': single images of one telescope type
    #   - 'stereo': stereoscopic images of events
    mode: 'mono'

    # List of telescope types to load.
    # Valid telescope types:
    #   - 'LST_LST_LSTCam': LST Telescope and Camera
    #   - 'LST_MAGIC_MAGICCam': MAGIC Telescopes
    #   - 'MST_MST_FlashCam': MST Telescope with FlashCAM Camera
    #   - 'MST_MST_NectarCam': MST Telescope with NectarCAM Camera
    #   - 'SST_1M_DigiCam4': SST1M Telescope with DigiCam Camera
    #   - 'SST_SCT_SCTCam': SCT Telescope and Camera
    #   - 'SST_ASTRI_ASTRICam': SST-2M ASTRI Telescope and CHEC Camera
    # Camera images from the following telescopes can be read, but writers
    # for the data are not yet available:
    #   - FACT, HESS-I, HESS-II, VERITAS
    # Required list of strings.
    selected_telescope_types: ['LST_LST_LSTCam']

    # Subset of telescope IDs to include of the selected telescope types.
    # Optional list of integer values.
    # Default: load all telescopes of the selected telescope types.
    selected_telescope_ids: [1, 2, 3, 4]

    # Dict of multiplicity selection.
    # Valid options:
    #     - 'telescope_types': int (from above)
    #     - 'Subarray': int (for full array multiplicity)
    # Optinal dict. Not needed for MAGIC data!
    #multiplicity_selection: {"LST_LST_LSTCam": 4}

    # Dict of quality selections from the DL1 file.
    # The recommended way of applying quality cuts to the data.
    # Optional dict.
    quality_selection:
        - {col_name: "hillas_intensity", min_value: 50.0}
        - {col_name: "leakage_intensity_width_2", max_value: 0.2}

    # Whether to shuffle the examples.
    # Optional Boolean. Default: False
    shuffle: false

    # Seed for shuffling the examples. Only used if shuffle is true.
    # Optional integer. Default: use default random initialization.
    seed: 1234

    # Waveform settings to load.
    # Optional dicts. Default: None
    #  - Valid options for 'waveform_type' are 'raw' (R0) or 'calibrated' (R1).
    #  - 'waveform_sequence_length' is an integer corresponding to the length
    #    of the sequence around the shower maximum
    #  - Valid options for 'waveform_format' are 'timechannel_first' or 'timechannel_last'.
    #  - 'waveform_r0pedsub' is a boolean weather to calculate and subtract the pedestal level
    #    per pixel outside the sequence aound the shower maximum (only selectable for R0).
    waveform_settings:
            type: 'raw'
            sequence_length: 5
            r0pedsub: True

    # Image settings to load.
    # Optional dicts. Default: None
    #  - Valid options for 'image_channels' are 'image', 'peak_time' 'cleaned_image' and 'clean_peak_time',
    #    or whichever columns are in the telescope tables of your data files.
    image_settings:
        image_channels: ['image', 'peak_time']

    # Parameter settings to load.
    # Optional dicts. Default: None
    #  - Valid options for 'parameter_list' are 'image', 'peak_time' 'cleaned_image' and 'clean_peak_time',
    #    or whichever columns are in the telescope tables of your data files.
    #parameter_settings:
    #    parameter_list: ['hillas_intensity', 'leakage_intensity_width_2']

    # Settings passed directly as arguments to ImageMapper.
    # Optional dictionary. Default: {}
    #
    # The camera types supported by ImageMapper that can be used in
    # the settings below are 'ASTRICam', 'CHEC', 'DigiCam', 'FACT',
    # 'FlashCam', 'HESS-I', 'HESS-II', 'LSTCam', 'MAGICCam', 'NectarCam',
    # 'SCTCam', and 'VERITAS'.
    mapping_settings:

        # List of camera types to calculate mapping tables for.
        # Optional list of camera type strings.
        # Default: calculate mapping tables for all supported camera types
        camera_types: ['LSTCam']

        # Algorithm to convert image vectors to rectangular arrays
        # Optional dictionary with string keys and string values.
        # Default: 'oversampling' for all camera types not in the dictionary
        # Valid keys are any camera types supported by ImageMapper.
        # Valid mapping method options (values):
        #   - 'oversampling': Convert each hexagonal pixel to four square
        #       pixels and reweight the charge to realign to a square grid
        #   - 'rebinning': Interpret the intensity as a histogram and rebin
        #       the hexagonal histogram into a square one.
        #   - 'nearest_interpolation': Perform nearest neighbor
        #   - 'bilinear_interpolation': Draw Delaunay triangulation and
        #       perform bilinear interpolation
        #   - 'bicubic_interpolation': Draw Delaunay triangulation and
        #       perform bicubic interpolation
        #   - 'image_shifting': Shift pixels of alternate rows (hexagonal
        #       cameras only)
        #   - 'axial_addressing': Warp images by relabeling axes (hexagonal
        #       cameras only)
        mapping_method:
            'LSTCam': 'bilinear_interpolation'
            'FlashCam': 'bilinear_interpolation'
            'NectarCam': 'bilinear_interpolation'
            'DigiCam': 'bilinear_interpolation'
            'SCTCam': 'oversampling'
            'CHEC': 'oversampling'
            'MAGICCam': 'bilinear_interpolation'

        # Number of padding pixels to add around each edge of the mapped image.
        # A value of 1 will increase both the image height and width by 2, etc.
        # Optional dictionary with camera type string keys and integer values.
        # Default: 0 for any camera type not specified
        padding:
            'LSTCam': 2
            'FlashCam': 1
            'NectarCam': 2
            'DigiCam': 2
            'SCTCam': 0
            'CHEC': 0
            'MAGICCam': 2

        # Shape of mapped image to return. Used only for mapping methods
        # without a fixed output shape: rebinning, nearest interpolation,
        # bilinear interpolation, bicubic interpolation
        # Optional dictionary with camera type string keys and values that
        # are lists of three integers: (<width>, <length>, <num_channels>)
        interpolation_image_shape:
            'LSTCam': [110, 110, 1]

        # Explicitly set output grid points outside the camera to 0
        # when performing bilinear and bicubic interpolation.
        # Optional Boolean. Default: false
        mask_interpolation: false

    # Auxiliary information to return from the Array Information table.
    # Optional list of strings. Default: return no array info
    # Valid options are columns in the Array_Information table.
    #array_info: ['x', 'y', 'z']

    # Auxiliary information to return from the Events table.
    # Optional list of strings. Default: return no event info
    # Valid options are columns in the Events table.
    # These options have to be in consonance with the 'label_names' in 'Model'.
    event_info: ['true_shower_primary_id', 'true_energy', 'true_alt', 'true_az']

    # Transforms to apply to the data, in order.
    # Optional list of dictionaries with the following key/value pairs:
    #   - 'path': Optional string. Path to module containing Transform.
    #             Default: path to DL1DataHandler
    #   - 'module': Optional string. Name of module containing Transform.
    #               Default: 'processor'
    #   - 'name': Required string. Name of Transform.
    #   - 'args': Optional dictionary. Arguments to pass to transform using
    #             Transform(**args). Default: {}
    # Valid transform names are those defined in DL1DataHandler or any defined
    # in the specified path and module.
    transforms:
        -
            name: 'MCEnergy'
    #   -
    #       path: '/path/to/my/module/'
    #       module: 'my_transform'
    #       name: 'MyTransform'
    #       args: {'arg1': 'foo', 'arg2': 3.14}

    # Whether to validate that the shape and dtype of the processed data
    # matches the example description.
    # Recommended setting is True while developing the network and False
    # during production.
    # Optional Boolean. Default: False
    validate_processor: False

# Settings for the Keras batch generator.
Input:

    # Number of consecutive examples to combine into a batch for a single gpu.
    # Optional integer. Default: 1
    batch_size_per_worker: 32

    # Whether to stack the stereo images from an event to feed it
    # to a monoscopic model.
    # Optional boolean. Default: false
    stack_telescope_images: false

# Settings for the TensorFlow model. The options in this and the
# Model Parameters section are passed to the Estimator model_fn
# and the user's model function.
Model:

    # Path to directory containing model module.
    # Optional string or null. Default if missing or null is to load
    # CTLearn default models directory: 'installation_path/ctlearn/ctlearn/default_models/'
    #model_directory: '/my/model/path/'

    # Name of the constructed model.
    # Optional string or null.
    name: 'SingleCNN'

    # Required dictionary containing a module/function pair.
    # Module in model directory and function in module implementing the model.
    # Module and function pairs included with CTLearn:
    #   - {module: 'single_cnn', function: 'single_cnn_model'}: monoscopic
    #       model using a basic convolutional neural network (CNN)
    #       or residual blocks (ResNets)
    #   - {module: 'cnn_rnn', function: 'cnn_rnn_model'}: array model
    #       feeding output of a basic CNN or a ResNet for each  telescope
    #       into a recurrent neural network (RNN)
    backbone: {module: 'single_cnn', function: 'single_cnn_model'}

    # Required dict with keys 'module' and 'function' & string values.
    # Module and function for the CTLearn backbone.
    # Valid options:
    #   - {module: 'basic', function: 'conv_block'}
    #   - {module: 'resnet', function: 'stacked_res_blocks'}
    engine: {module: 'basic', function: 'conv_block'}

    # Optional arguments for the first Conv2D and MaxPool layer
    # before the selected model.
    #init_layer: {filters: 64, kernel_size: 7, strides: 1}
    #init_max_pool: {size: 3, strides: 2}

    # Required dict with keys 'module' and 'function' & string values.
    # Module and function for the CTLearn head.
    # Valid options:
    #   - {module: 'head', function: 'standard_head'}
    head: {module: 'head', function: 'standard_head'}

    # Required string or null.
    # Path to a checkpoint file or model directory from which to load
    # pretrained network weights. If null, don't load any weights.
    pretrained_weights: null

    # Optional float. Default: 0.5
    # Dropout rate of dropout layers in the CNNRNN model.
    dropout_rate: 0.5

    # Optional boolean. Default: false
    # Plot Keras model
    plot_model: false

    # Optional boolean. Default: false
    # Save Keras model to ONNX format
    save2onnx: false


# The options in this and the Model section are passed in a dictionary
# called "params" to the model function.
# The config option 'model_directory' above is included in params as well.
# None of these options are accessed anywhere outside the model, so
# arbitrary options may be added here. This permits custom configuration
# of user-provided models.
# Below are listed the configuration options for the CTLearn included
# models, which are only accessed if using those models.
Model Parameters:

    basic:
        conv_block:
            # Required list of dicts with keys 'filters', 'kernel_size' and number
            # with integer values.
            # If number value is not stored will be automatically set to 1
            # Filter dimension and kernel size for the CNN block
            # convolutional layers.
            # Number for the desired number of convolutional layers 
            layers:
                - {filters: 32, kernel_size: 3, number: 1}
                - {filters: 64, kernel_size: 3, number: 1}
                - {filters: 128, kernel_size: 3, number: 1}

            # Required dictionary with keys 'size' and 'strides' and
            # integer values, or null.
            # Max pool size and strides. If null, don't perform any pooling.
            max_pool: {size: 2, strides: 2}

            # Required integer or null.
            # Number of output filters of a final 1x1 convolutional layer.
            # If null, don't include this bottleneck layer.
            bottleneck: null

            # Optional Boolean. Default: false
            # Whether to include a batch normalization layer after each
            # convolutional layer. Exercise caution when using with
            # array-level models.
            batchnorm: false

        fully_connect:
            # Required list of integers.
            # Number of units for each fully-connected head dense layer.
            layers: [1024, 512, 256, 128, 64]

            # Optional Boolean. Default: false
            # Whether to include a batch normalization layer after each
            # dense layer.
            batchnorm: false

        conv_head:
            # Required list of dicts with keys 'filters' and 'kernel_size'
            # with integer values.
            # Filter dimension and kernel size for the convolutional
            # network head layers.
            layers:
                - {filters: 64, kernel_size: 3}
                - {filters: 128, kernel_size: 3}
                - {filters: 256, kernel_size: 3}

            # Optional Boolean. Default: true
            # Whether to perform average pooling over spatial dimensions
            # of the convolutional output before calculating final output.
            final_avg_pool: true

            # Optional Boolean. Default: false
            # Whether to include a batch normalization layer after each
            # convolutional layer.
            batchnorm: false

        # Optional float. Default: 0.99
        # Decay parameter for batch normalization layers.
        batchnorm_decay: 0.99

    resnet:
        stacked_res_blocks:
            # Optional string. Default: 'bottleneck'
            # Type of the residual block.
            # Valid options:
            #   - 'basic'
            #   - 'bottleneck'
            residual_block: 'bottleneck'

            # Required list of dicts with keys 'filters' and 'blocks'
            # with integer values.
            # Filter dimension and number of blocks for the ResNet 
            # architecture.
            # Typical ResNet architectures:
            # Thin-ResNet
            #- {filters: 48, blocks: 2}
            #- {filters: 96, blocks: 3}
            #- {filters: 128, blocks: 3}
            #- {filters: 256, blocks: 3}
            # ResNet50
            #- {filters: 64, blocks: 3}
            #- {filters: 128, blocks: 4}
            #- {filters: 256, blocks: 6}
            #- {filters: 512, blocks: 3}
            architecture:
                - {filters: 48, blocks: 2}
                - {filters: 96, blocks: 3}
                - {filters: 128, blocks: 3}
                - {filters: 256, blocks: 3}

    # Dictionary of tasks where the values contain the fully connected layers
    # head of each reconstruction task. The final loss is calculated with a weighted sum 
    # over the individual losses. Currently the weights are constanst values.
    # For the classification task, the class names are passed into the dict
    # in order of class label (0 to n-1).
    standard_head:
        type: {class_names: ['proton', 'gamma'], fc_head: [512, 256, 2], weight: 1.0}
        energy: {fc_head: [512, 256, 1], weight: 1.0}
        direction: {fc_head: [512, 256, 2], weight: 1.0}

    # Optional dict of attention mechanisms.
    # Valid options:
    #   Dual attention (Spatial and channel squeeze-and-excitation attention)
    #   - {mechanism: 'Squeeze-and-Excitation', ratio: 16}
    #   Channel Squeeze-and-excitation attention
    #   - {mechanism: 'Channel-Squeeze-and-Excitation', ratio: 4}
    #   Spatial squeeze-and-excitatiom attention
    #   - {mechanism: 'Spatial-Squeeze-and-Excitation'}
    # References:
    # - [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)
    # - [Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)
    attention: {mechanism: 'Squeeze-and-Excitation', ratio: 16}

Training:

    # Optional float. Default: 0.1
    # Randomly chosen fraction of data to set aside for validation.
    validation_split: 0.1

    # Required number of epochs on which to train.
    num_epochs: 20

    # Required string.
    # Valid options: ['Adadelta', 'Adam', 'RMSProp', 'SGD']
    # Optimizer function for training.
    optimizer: 'Adam'

    # Optional float. Required if optimizer is 'Adam', ignored otherwise.
    # Epsilon parameter for the Adam optimizer.
    adam_epsilon: 1.0e-8

    # Required integer.
    # Base learning rate before scaling or annealing.
    base_learning_rate: 0.001

    # Optional integer and floats.
    # Reducing the learning rate after the network stops learning.
    lr_reducing_patience: 5
    lr_reducing_factor: 0.5
    lr_reducing_mindelta: 0.01
    lr_reducing_minlr: 0.00001

Prediction:

    # Required string if running in predict mode.
    # Name of the file to save predictions.
    prediction_file: ./ctlearn_prediction.h5

    # A dict of either a list of data file paths,
    # or a path to a file listing data file paths, one per line.
    # Required dict of list or string if running in predict mode.
    prediction_file_lists: 
        prediction_file_list1.h5: /tmp/dataset/prediction_file_list1.txt
        prediction_file_list2.h5: /tmp/dataset/prediction_file_list2.txt
