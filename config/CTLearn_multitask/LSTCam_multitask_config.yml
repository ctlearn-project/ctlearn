# Example config file containing all possible options for CTLearn, whether
# they're required or optional, their datatypes, and default arguments.
#
# For an explanation of the data format and valid column names for the
# selection_string, array_info, and event_info Data config options, see
# https://github.com/cta-observatory/dl1-data-handler/wiki/CTA-ML-Data-Format
# or use ViTables http://vitables.org/ to examine the HDF5 files directly.
---

Logging:
    # Path to directory to store TensorFlow model checkpoints and summaries.
    # A timestamped copy of the configuration file will also be put here.
    # Required string.
    model_directory: '/home/tjark/multitask/testrun'

Data:
    # Either a list of data file paths,
    # or a path to a file listing data file paths, one per line.
    # Required list or string.
    file_list: '/home/tjark/multitask/new_data.txt'

    # Type of examples to load.
    # Optional string. Default: 'mono'
    # Valid options:
    #   - 'mono': single images of one telescope type
    #   - 'stereo': events of one telescope type
    #   - 'multi-stereo': events including multiple telescope types
    mode: 'mono'

    # Telescope type or list thereof to load.
    # Optional string (mono or stereo) or list of strings (multi-stereo).
    # Default in mono or stereo mode: first telescope type in the dataset
    # Default in multi-stereo mode: all telescope types in the dataset
    # Valid telescope types:
    #   - 'LST_LST_LSTCam': LST Telescope and Camera
    #   - 'MST_FlashCam': MST Telescope with FlashCAM Camera
    #   - 'MST_NectarCam': MST Telescope with NectarCAM Camera
    #   - 'SCT_SCTCam': SCT Telescope and Camera
    #   - 'SST1M_DigiCam': SST-1M Telescope with DigiCam Camera
    #   - 'SSTASTRI_ASTRICam': SST-2M ASTRI Telescope and Camera
    #   - 'SSTGCT_CHEC': SST-2M GCT Telescope with CHEC Camera
    # Camera images from the following telescopes can be read, but writers
    # for the data are not yet available:
    #   - FACT, HESS-I, HESS-II, MAGIC, VERITAS
    selected_telescope_type: 'LST_LST_LSTCam'
   
    # Subset of telescope IDs to include of the selected telescope types.
    # Optional dictionary with string keys and list of integer values.
    # Valid keys are the telescope types listed above, and valid values are
    # are the tel_ids of the telescopes to load of that telescope type.
    # Default if not provided or telescope type not in dict: load all
    # telescopes of that telescope type if selected.
    selected_telescope_ids:
        'LST_LST_LSTCam': [1, 2, 3, 4]

    # PyTables cut condition to apply selection cuts to events.
    # Optional string. Default: don't apply any cuts
    # See https://www.pytables.org/usersguide/condition_syntax.html
    # for explanation of syntax.
    selection_string: '(mc_energy > 1.0) & (h_first_int < 20000)'

    # Event selection filters to apply to each data file.
    # Optional list of dictionaries with the following key/value pairs:
    #   - 'path': Optional string. Path to module containing filter.
    #             Default: path to DL1DataHandler
    #   - 'module': Optional string. Name of module containing filter.
    #               Default: 'utils'
    #   - 'name': Required string. Name of filter.
    #   - 'args': Optional dictionary. Arguments to pass to filter using
    #             filter(file, **args). Default: {}
    # Valid filter names are those defined in DL1DataHandler or any defined
    # in the specified path and module.
    event_selection:
        -
            name: 'event_intensity_filter'
            args: {i_min: 100, i_max: 316}
#       -
#           path: '/path/to/my/module/'
#           module: 'my_filter'
#           name: 'filter_fn'
#           args: {'arg1': 'foo', 'arg2': 3.14}

    # Image selection filters to apply to each image (mono mode only).
    # Optional list of dictionaries with the following key/value pairs:
    #   - 'path': Optional string. Path to module containing filter.
    #             Default: path to DL1DataHandler
    #   - 'module': Optional string. Name of module containing filter.
    #               Default: 'utils'
    #   - 'name': Required string. Name of filter.
    #   - 'args': Optional dictionary. Arguments to pass to filter using
    #             filter(file, **args). Default: {}
    # Valid filter names are those defined in DL1DataHandler or any defined
    # in the specified path and module.
    image_selection:
        -
            name: 'image_intensity_filter'
            args: {i_min: 10, i_max: 31.6}
#       -
#           path: '/path/to/my/module/'
#           module: 'my_filter'
#           name: 'filter_fn'
#           args: {'arg1': 'foo', 'arg2': 3.14}

    # Whether to shuffle the examples.
    # Optional Boolean. Default: False
    shuffle: false

    # Seed for shuffling the examples. Only used if shuffle is true.
    # Optional integer. Default: use default random initialization.
    seed: 1234

    # Image channels to load.
    # Optional list of strings. Default: ['charge']
    # Valid options are 'charge' and 'peakpos', or whichever columns are in
    # the telescope tables of your data files.
    image_channels: ['charge']

    # Settings passed directly as arguments to ImageMapper.
    # Optional dictionary. Default: {}
    #
    # The camera types supported by ImageMapper that can be used in
    # the settings below are 'ASTRICam', 'CHEC', 'DigiCam', 'FACT',
    # 'FlashCam', 'HESS-I', 'HESS-II', 'LSTCam', 'MAGICCam', 'NectarCam',
    # 'SCTCam', and 'VERITAS'.
    mapping_settings:

        # List of camera types to calculate mapping tables for.
        # Optional list of camera type strings.
        # Default: calculate mapping tables for all supported camera types
        camera_types: ['LSTCam']

        # Algorithm to convert image vectors to rectangular arrays
        # Optional dictionary with string keys and string values.
        # Default: 'oversampling' for all camera types not in the dictionary
        # Valid keys are any camera types supported by ImageMapper.
        # Valid mapping method options (values):
        #   - 'oversampling': Convert each hexagonal pixel to four square
        #       pixels and reweight the charge to realign to a square grid
        #   - 'rebinning': Interpret the intensity as a histogram and rebin
        #       the hexagonal histogram into a square one.
        #   - 'nearest_interpolation': Perform nearest neighbor
        #   - 'bilinear_interpolation': Draw Delaunay triangulation and
        #       perform bilinear interpolation
        #   - 'bicubic_interpolation': Draw Delaunay triangulation and
        #       perform bicubic interpolation
        #   - 'image_shifting': Shift pixels of alternate rows (hexagonal
        #       cameras only)
        #   - 'axial_addressing': Warp images by relabeling axes (hexagonal
        #       cameras only)
        mapping_method:
            'LSTCam': 'bilinear_interpolation'

        # Number of padding pixels to add around each edge of the mapped image.
        # A value of 1 will increase both the image height and width by 2, etc.
        # Optional dictionary with camera type string keys and integer values.
        # Default: 0 for any camera type not specified
        padding:
            'LSTCam': 2
            'FlashCam': 1
            'NectarCam': 2
            'SCTCam': 4
            'DigiCam': 1
            'ASTRICam': 0
            'CHEC': 0
            'VERITAS': 1

        # Shape of mapped image to return. Used only for mapping methods
        # without a fixed output shape: rebinning, nearest interpolation,
        # bilinear interpolation, bicubic interpolation
        # Optional dictionary with camera type string keys and values that
        # are lists of three integers: (<width>, <length>, <num_channels>)
        interpolation_image_shape:
            'LSTCam': [110, 110, 1]

        # Explicitly set output grid points outside the camera to 0
        # when performing bilinear and bicubic interpolation.
        # Optional Boolean. Default: false
        mask_interpolation: false

    # Auxiliary information to return from the Array Information table.
    # Optional list of strings. Default: return no array info
    # Valid options are columns in the Array_Information table.
    array_info: ['x', 'y', 'z']

    # Auxiliary information to return from the Events table.
    # Optional list of strings. Default: return no event info
    # Valid options are columns in the Events table.
    event_info: ['shower_primary_id', 'mc_energy', 'alt', 'az', 'core_x', 'core_y', 'x_max']

    # Transforms to apply to the data, in order.
    # Optional list of dictionaries with the following key/value pairs:
    #   - 'path': Optional string. Path to module containing Transform.
    #             Default: path to DL1DataHandler
    #   - 'module': Optional string. Name of module containing Transform.
    #               Default: 'processor'
    #   - 'name': Required string. Name of Transform.
    #   - 'args': Optional dictionary. Arguments to pass to transform using
    #             Transform(**args). Default: {}
    # Valid transform names are those defined in DL1DataHandler or any defined
    # in the specified path and module.
    transforms:
        -
            name: 'ConvertShowerPrimaryIDToClassLabel'
        -
            name: 'NormalizeTelescopePositions'
            args: {'norm_x': 1.0, 'norm_y': 1.0, 'norm_z': 1.0}
#       -
#           path: '/path/to/my/module/'
#           module: 'my_transform'
#           name: 'MyTransform'
#           args: {'arg1': 'foo', 'arg2': 3.14}

    # Whether to validate that the shape and dtype of the processed data
    # matches the example description.
    # Recommended setting is True while developing the network and False
    # during production.
    # Optional Boolean. Default: False
    validate_processor: False

# Settings for the TensorFlow Estimator input_fn.
Input:

    # Random seed for shuffling the dataset.
    # Optional integer. Default: use default TensorFlow behavior
    seed: null

    # Number of consecutive examples to combine into a batch.
    # Optional integer. Default: 1
    batch_size: 32

    # Number of examples to load into memory before shuffling.
    # For valid results, either randomize the example order
    # in the generator (i.e. set Data:shuffle = True) or set to
    # greater than or equal to the number of examples in the dataset.
    # For best performance, set to less than the total number of examples.
    # Optional integer. Default: All examples in dataset
    shuffle_buffer_size: 10000

    # Maximum number of batches to be buffered when prefetching.
    # Optional integer. Default: 1
    prefetch_buffer_size: 8

# Settings for the TensorFlow model. The options in this and the
# Model Parameters section are passed to the Estimator model_fn
# and the user's model function.
Model:

    # Optional string or null. Default if missing or null is to load
    # CTLearn default models directory: 'ctlearn/ctlearn/default_models/'
    # Path to directory containing model module.
    model_directory: '/home/tjark/multitask/ctlearn/ctlearn/default_models/'
    
    # Required dictionary containing a module/function pair.
    # Module in model directory and function in module implementing the model.
    # Module and function pairs included with CTLearn:
    #   - {module: 'vanilla_classification', function: 'vanilla_classification_model'}:
    #       This model is implemented for a cross check. We want/need to reproduce
    #       the CTLearn v0.4.0 results. After the cross check this can be deleted,
    #       because the 'vanilla' model should give the same results, when selecting
    #       only 'gammahadron_classification' in 'learning_tasks'. I need to check, if
    #       we can only pass in one head in the multi_head class!
    #   - {module: 'vanilla', function: 'vanilla_model'}:
    #       The 'vanilla' model is calling the single_tel submodel to create the CNN block.
    #       After the CNN block the output is flatten and feed to several dense layers,
    #       depending on the selected learning tasks.
    #   - {module: 'gammaPhysNet', function: 'gammaPhysNet_model'}:
    #       TODO: Implement this model
    #   - {module: 'gammaPhysNet2', function: 'gammaPhysNet2_model'}:
    #       TODO: Implement this model
    #   - {module: 'gammaPhysNetS', function: 'gammaPhysNetS_model'}:
    #       TODO: Implement this model
    #   - {module: 'gammaPhysNet2S', function: 'gammaPhysNet2S_model'}:
    #       TODO: Implement this model
    #
    model: {module: 'vanilla', function: 'vanilla_model'}
    
    # Optional array of strings.
    # This array is only used, if the 'vanilla' model is selected.
    # If the array has more than one element 'Multitask Learning' (MTL)
    # is performed. Otherwise the network is learning a single task.
    # Valid options:
    #   - 'gammahadron_classification': Include gamma/hadron classification
    #       in the network
    #   - 'energy_regression': Include energy estimation in the network
    #   - 'direction_regression': Include arrival direction estimation
    #       in the network
    #   - 'impact_regression': Include impact parameter estimation
    #       in the network
    #   - 'showermaximum_regression': Include location of shower maximum
    #       estimation in the network
    learning_tasks: ['gammahadron_classification', 'energy_regression', 'direction_regression', 'impact_regression', 'showermaximum_regression']

    # Dictionary of labels where the keys are the label names.
    # For a classification label, the value is a list of class names
    # in order of class label (0 to n-1).
    # For a regression label, the value is null.
    # Optional dictionary with string keys and values either null or
    # a list of strings. Default: {}
    label_names:
        class_label:
            - 'proton'
            - 'gamma'

# The options in this, the Model section and the Learning Parameters section are
# passed in a dictionary called "params" to the model function.
# The config option 'model_directory' and 'modelfn_directory' above is included
# in params as well. None of these options are accessed anywhere outside the model,
# so arbitrary options may be added here. This permits custom configuration
# of user-provided models.
# Below are listed the configuration options for the CTLearn included
# models, which are only accessed if using those models.
Model Parameters:
    single_tel:
        # Required dict with keys 'module' and 'function' & string values.
        # Module and function for image classification network.
        # Valid options:
        #   - {module: 'basic', function: 'conv_block'}
        network: {module: 'basic', function: 'conv_block'}

        # Required string or null.
        # Path to a checkpoint file or model directory from which to load
        # pretrained network weights. If null, don't load any weights.
        pretrained_weights: null

    cnn_rnn:
        # Required dict with keys 'module' and 'function' & string values.
        # Module and function for single telescope CNN.
        # Valid options:
        #   - {module: 'basic', function: 'conv_block'}
        cnn_block: {module: 'basic', function: 'conv_block'}

        # Required string or null.
        # Path to a checkpoint file or model directory from which to load
        # pretrained CNN block weights. If null, don't load any weights.
        pretrained_weights: null

        # Optional float. Default: 0.5
        # Dropout rate of dropout layers.
        dropout_rate: 0.5

    variable_input_model:
        # Required dict with keys 'module' and 'function' & string values.
        # Module and function for single telescope CNN.
        # Valid options:
        #   - {module: 'basic', function: 'conv_block'}
        cnn_block: {module: 'basic', function: 'conv_block'}

        # Required string.
        # Valid options:
        #   - 'vector': Use for network heads expecting an input vector
        #   - 'feature_maps': Use for networks heads expecting input as
        #       a stack of feature maps as used for convolutional layers
        telescope_combination: 'feature_maps'

        # Required dict with keys 'module' and 'function' & string values.
        # Module and function for network head.
        # Valid options:
        #   - {module: 'basic', function: 'fc_head'}
        #   - {module: 'basic', function: 'conv_head'}
        network_head: {module: 'basic', function: 'conv_head'}

        # Required string or null.
        # Path to a checkpoint file or model directory from which to load
        # pretrained CNN block weights. If null, don't load any weights.
        pretrained_weights: null

    basic:
        conv_block:
            # Required list of dicts with keys 'filters' and 'kernel_size'
            # with integer values.
            # Filter dimension and kernel size for the CNN block
            # convolutional layers.
            layers:
                - {filters: 32, kernel_size: 3}
                - {filters: 64, kernel_size: 3}
                - {filters: 128, kernel_size: 3}

            # Required dictionary with keys 'size' and 'strides' and
            # integer values, or null.
            # Max pool size and strides. If null, don't perform any pooling.
            max_pool: {size: 2, strides: 2}

            # Required integer or null.
            # Number of output filters of a final 1x1 convolutional layer.
            # If null, don't include this bottleneck layer.
            bottleneck: null

            # Optional Boolean. Default: false
            # Whether to include a batch normalization layer after each
            # convolutional layer. Exercise caution when using with
            # array-level models.
            batchnorm: false

        fc_head:
            # Required list of integers.
            # Number of units for each fully-connected head dense layer.
            layers: [1024, 512, 256, 128, 64]

            # Optional Boolean. Default: false
            # Whether to include a batch normalization layer after each
            # dense layer.
            batchnorm: false

        conv_head:
            # Required list of dicts with keys 'filters' and 'kernel_size'
            # with integer values.
            # Filter dimension and kernel size for the convolutional
            # network head layers.
            layers:
                - {filters: 64, kernel_size: 3}
                - {filters: 128, kernel_size: 3}
                - {filters: 256, kernel_size: 3}

            # Optional Boolean. Default: true
            # Whether to perform average pooling over spatial dimensions
            # of the convolutional output before calculating final output.
            final_avg_pool: true

            # Optional Boolean. Default: false
            # Whether to include a batch normalization layer after each
            # convolutional layer.
            batchnorm: false

        # Optional float. Default: 0.99
        # Decay parameter for batch normalization layers.
        batchnorm_decay: 0.99
   
Training:

    # Optional float. Default: 0.1
    # Randomly chosen fraction of data to set aside for validation.
    validation_split: 0.1

    # Required integer.
    # Number of validations made before finishing training. If 0, run forever.
    num_validations: 0

    # Required integer.
    # Number of training steps to run before each evaluation
    # on the validation set.
    num_training_steps_per_validation: 1000

    # Required string.
    # Valid options: ['Adadelta', 'Adam', 'RMSProp', 'SGD']
    # Optimizer function for training.
    optimizer: 'Adam'

    # Optional float. Required if optimizer is 'Adam', ignored otherwise.
    # Epsilon parameter for the Adam optimizer.
    adam_epsilon: 1.0e-8

    # Required integer.
    # Base learning rate before scaling or annealing.
    base_learning_rate: 0.001

    # Required Boolean.
    # Whether to scale the learning rate inversely proportional to the
    # number of triggered telescopes. Not used for single tel models.
    scale_learning_rate: false

    # Required Boolean.
    # Whether to weight the loss to rebalance unequal classes.
    apply_class_weights: false

    # Required string or null. If provided, train on only variables
    # within the scope matching this name, freezing all others. This is
    # useful when loading pretrained weights. If null, train on all
    # trainable variables.
    variables_to_train: null

Prediction:
    # Save the true labels along with the predictions.
    # Optional Boolearn. Default: false
    save_labels: false

    # Save the example identifiers along with the predictions.
    # Mono mode: obs_id, event_id, tel_id
    # Stereo or multi-stereo mode: obs_id, event_id
    # Optional Boolearn. Default: false
    save_identifiers: false

    # Optional Boolean. Default: false
    # Whether to export predictions as a CSV file.
    export_as_file: false

    # Required Boolean if running in predict mode and export_as_file is true,
    # ignored otherwise.
    # Path to file to save predictions.
    prediction_file_path: '/home/tjark/multitask/testpredictions.csv'

TensorFlow:
    # Optional Boolean. Default: false
    # Whether to run TensorFlow debugger.
    run_TFDBG: false
